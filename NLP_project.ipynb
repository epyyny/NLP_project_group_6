{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/epyyny/NLP_project_group_6/blob/main/NLP_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7ccd44-9991-4313-a339-34edf72b0e04",
      "metadata": {
        "id": "7c7ccd44-9991-4313-a339-34edf72b0e04"
      },
      "outputs": [],
      "source": [
        "#remember to install spacy and en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2425b8a6-7912-42b2-b5e4-6066ee37d681",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2425b8a6-7912-42b2-b5e4-6066ee37d681",
        "outputId": "ec831c5e-b2f7-4873-986d-1e23f69b1772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     tweet_id   sentiment       author  \\\n",
            "0  1956967341       empty   xoshayzers   \n",
            "1  1956967666     sadness    wannamama   \n",
            "2  1956967696     sadness    coolfunky   \n",
            "3  1956967789  enthusiasm  czareaquino   \n",
            "4  1956968416     neutral    xkilljoyx   \n",
            "\n",
            "                                             content  \n",
            "0  @tiffanylue i know  i was listenin to bad habi...  \n",
            "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
            "2                Funeral ceremony...gloomy friday...  \n",
            "3               wants to hang out with friends SOON!  \n",
            "4  @dannycastillo We want to trade with someone w...  \n"
          ]
        }
      ],
      "source": [
        "#get the dataset with pandas\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/abishekarun/Text-Emotion-Classification/master/text_emotion.csv\"\n",
        "df = pd.read_csv(url)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3ec0ffd5-063e-4f2c-b697-72e60e7aed7b",
      "metadata": {
        "id": "3ec0ffd5-063e-4f2c-b697-72e60e7aed7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b99700-2fbc-4433-8e6f-87285f4ce52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category: sadness\n",
            "     tweet_id sentiment       author  \\\n",
            "0  1956967666   sadness    wannamama   \n",
            "1  1956967696   sadness    coolfunky   \n",
            "2  1956968487   sadness     ShansBee   \n",
            "3  1956969035   sadness  nic0lepaula   \n",
            "4  1956969172   sadness   Ingenue_Em   \n",
            "\n",
            "                                             content  \n",
            "0  Layin n bed with a headache  ughhhh...waitin o...  \n",
            "1                Funeral ceremony...gloomy friday...  \n",
            "2  I should be sleep, but im not! thinking about ...  \n",
            "3            @charviray Charlene my love. I miss you  \n",
            "4         @kelcouch I'm sorry  at least it's Friday?  \n",
            "\n",
            "\n",
            "Category: enthusiasm\n",
            "     tweet_id   sentiment        author  \\\n",
            "0  1956967789  enthusiasm   czareaquino   \n",
            "1  1956981427  enthusiasm       Caillie   \n",
            "2  1957066701  enthusiasm     FinIsKing   \n",
            "3  1957067779  enthusiasm  Maureen12683   \n",
            "4  1957073668  enthusiasm       kort030   \n",
            "\n",
            "                                             content  \n",
            "0               wants to hang out with friends SOON!  \n",
            "1  bed...sorta. today was good, sara has strep th...  \n",
            "2                                I want another tatt  \n",
            "3         So, I need to make a lot of money tomorrow  \n",
            "4  @lilxamyx08 i know ridiculous! we never got to...  \n",
            "\n",
            "\n",
            "Category: neutral\n",
            "     tweet_id sentiment       author  \\\n",
            "0  1956968416   neutral    xkilljoyx   \n",
            "1  1956969456   neutral   feinyheiny   \n",
            "2  1956972116   neutral        jansc   \n",
            "3  1956975441   neutral  LovableKeKe   \n",
            "4  1956975860   neutral   analalalah   \n",
            "\n",
            "                                             content  \n",
            "0  @dannycastillo We want to trade with someone w...  \n",
            "1                                   cant fall asleep  \n",
            "2  No Topic Maps talks at the Balisage Markup Con...  \n",
            "3                          @cynthia_123 i cant sleep  \n",
            "4                    I missed the bl***y bus!!!!!!!!  \n",
            "\n",
            "\n",
            "Category: worry\n",
            "     tweet_id sentiment         author  \\\n",
            "0  1956968477     worry  xxxPEACHESxxx   \n",
            "1  1956968636     worry       mcsleazy   \n",
            "2  1956969531     worry   dudeitsmanda   \n",
            "3  1956971473     worry          LCJ82   \n",
            "4  1956971981     worry  andreagauster   \n",
            "\n",
            "                                             content  \n",
            "0  Re-pinging @ghostridah14: why didn't you go to...  \n",
            "1               Hmmm. http://www.djhero.com/ is down  \n",
            "2                            Choked on her retainers  \n",
            "3  @PerezHilton lady gaga tweeted about not being...  \n",
            "4  @raaaaaaek oh too bad! I hope it gets better. ...  \n",
            "\n",
            "\n",
            "Category: surprise\n",
            "     tweet_id sentiment        author  \\\n",
            "0  1956970860  surprise  okiepeanut93   \n",
            "1  1957005410  surprise        HIM357   \n",
            "2  1957008434  surprise     Just_Cath   \n",
            "3  1957008478  surprise    bekahjayne   \n",
            "4  1957008766  surprise     shutitoff   \n",
            "\n",
            "                                             content  \n",
            "0                                       Got the news  \n",
            "1  2 days of this month left, and I only have 400...  \n",
            "2  @Bern_morley where are you? In Bris? I can't h...  \n",
            "3  bec vs fat food   --- winner = fat food  but n...  \n",
            "4  I had a dream about a pretty pretty beach and ...  \n",
            "\n",
            "\n",
            "Category: love\n",
            "     tweet_id sentiment          author  \\\n",
            "0  1956971170      love    poppygallico   \n",
            "1  1956989093      love    Angela_Grace   \n",
            "2  1956996385      love          votech   \n",
            "3  1956998370      love   rockinchick11   \n",
            "4  1957002773      love  UncoolRockstar   \n",
            "\n",
            "                                             content  \n",
            "0                               @annarosekerr agreed  \n",
            "1  @RobertF3 correct! I ADORE him. I just plucked...  \n",
            "2  @freepbx sounds good. Appreciate the suggestio...  \n",
            "3  Pats in philly at 2 am. I love it. Mmm cheeses...  \n",
            "4  @NisforNeemah thanks neemah. I'm gonna be sooo...  \n",
            "\n",
            "\n",
            "Category: fun\n",
            "     tweet_id sentiment         author  \\\n",
            "0  1956972097       fun  schiz0phren1c   \n",
            "1  1956977187       fun     diNGUYEN31   \n",
            "2  1957001854       fun       Samiijoo   \n",
            "3  1957005696       fun      nickhalme   \n",
            "4  1957007268       fun       EmaMolly   \n",
            "\n",
            "                                             content  \n",
            "0  Wondering why I'm awake at 7am,writing a new s...  \n",
            "1  @DavidArchie &lt;3 your gonna be the first  tw...  \n",
            "2   RIP leonardo. You were a great mini fiddler crab  \n",
            "3  @IdleThumbs Up is out?  I didn't get the memo ...  \n",
            "4  @relly1  OMG Ur alive!!! LOL  2day has gone so...  \n",
            "\n",
            "\n",
            "Category: hate\n",
            "     tweet_id sentiment       author  \\\n",
            "0  1956974706      hate  MavrickAces   \n",
            "1  1956987600      hate    naterkane   \n",
            "2  1956989601      hate     M0anique   \n",
            "3  1956990288      hate  MissPassion   \n",
            "4  1956991009      hate      rdyfrde   \n",
            "\n",
            "                                             content  \n",
            "0  It is so annoying when she starts typing on he...  \n",
            "1  dammit! hulu desktop has totally screwed up my...  \n",
            "2  @ cayogial i wanted to come to BZ this summer ...  \n",
            "3                @mrgenius23 You win ... SIGH Rakeem  \n",
            "4                      @soviet_star Damn, that sucks  \n",
            "\n",
            "\n",
            "Category: happiness\n",
            "     tweet_id  sentiment       author  \\\n",
            "0  1956977084  happiness     ktierson   \n",
            "1  1956983874  happiness         joia   \n",
            "2  1956985535  happiness   quarrygirl   \n",
            "3  1956996765  happiness  rarmendariz   \n",
            "4  1957017522  happiness   Davislb921   \n",
            "\n",
            "                                             content  \n",
            "0  mmm much better day... so far! it's still quit...  \n",
            "1  So great to see Oin &amp; Cynthia.  So happy. ...  \n",
            "2  @havingmysay  dude, that is my favorite sandwi...  \n",
            "3  Need to pack for CALI CALI! Cannot waittt! Thi...  \n",
            "4  took a math test today. The day before the tes...  \n",
            "\n",
            "\n",
            "Category: boredom\n",
            "     tweet_id sentiment           author  \\\n",
            "0  1956993007   boredom         villa_ld   \n",
            "1  1957038475   boredom          kameezy   \n",
            "2  1957044366   boredom       snoopync18   \n",
            "3  1957083786   boredom     munkeysmomma   \n",
            "4  1957102562   boredom  Quastbabynumbr5   \n",
            "\n",
            "                                             content  \n",
            "0                                       i'm so tired  \n",
            "1                            Waiting in line @ tryst  \n",
            "2             why did i agree to work a double shift  \n",
            "3  is really, really bored... I guess I will go t...  \n",
            "4                       So deep its priecing my soul  \n",
            "\n",
            "\n",
            "Category: relief\n",
            "     tweet_id sentiment          author  \\\n",
            "0  1957001506    relief      benmfowler   \n",
            "1  1957014389    relief     misscinders   \n",
            "2  1957037860    relief          oh_slc   \n",
            "3  1957053409    relief  gangstamittens   \n",
            "4  1957062621    relief        antonywu   \n",
            "\n",
            "                                             content  \n",
            "0                                        I'm at work  \n",
            "1  is done painting all the bedroom furniture, I ...  \n",
            "2  Scary lightning and thunder  I'm glad it's ove...  \n",
            "3                     is home.  safely... but hungry  \n",
            "4  just finished 8 hours of Texas Life Insurance ...  \n",
            "\n",
            "\n",
            "Category: anger\n",
            "     tweet_id sentiment           author  \\\n",
            "0  1957083641     anger         elDi_irk   \n",
            "1  1957089935     anger            umi78   \n",
            "2  1957110088     anger        NayNay_Rt   \n",
            "3  1957289252     anger       crazy_erin   \n",
            "4  1957291305     anger  msfussybritches   \n",
            "\n",
            "                                             content  \n",
            "0                              fuckin'm transtelecom  \n",
            "1                   Working   But it's Fridaaaayyyyy  \n",
            "2                         Packing  I don't like it..  \n",
            "3  I tried to dye my hair and all i got was a blo...  \n",
            "4  &quot;locked up abroad&quot; makes bein half b...  \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#split the data into DataFrames based on sentiment categories\n",
        "\n",
        "#at this point we can remove the rows with \"empty\" tag depending on our desired resuls\n",
        "df = df[df[\"sentiment\"] != \"empty\"]\n",
        "\n",
        "#get all sentiment categories\n",
        "categories = df[\"sentiment\"].unique()\n",
        "#create a dictionary\n",
        "category_df = {}\n",
        "for category in categories:\n",
        "\n",
        "    #filter using current category\n",
        "    filtered_df = df[df['sentiment'] == category]\n",
        "\n",
        "    #reset the index of the filtered DataFrame\n",
        "    filtered_df = filtered_df.reset_index(drop=True)\n",
        "\n",
        "    #add the filtered DataFrame to the dictionary\n",
        "    category_df[category] = filtered_df\n",
        "\n",
        "\n",
        "for category, data in category_df.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    print(data.head())\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f6d3ad0c-45c2-4bf1-90be-389d70373410",
      "metadata": {
        "id": "f6d3ad0c-45c2-4bf1-90be-389d70373410"
      },
      "outputs": [],
      "source": [
        "#we now have the dataframe category_df\n",
        "\n",
        "#task 1:\n",
        "#suggest a script that constructs vocabulary set for each dataframe\n",
        "#begin by making functions needed in the script\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "#tokenize function\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())\n",
        "\n",
        "#modern english personal pronouns list\n",
        "pronouns = [\"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"them\", \"us\", \"him\", \"her\", \"his\", \"hers\", \"its\", \"theirs\", \"our\", \"your\"]\n",
        "def pronoun_counter(tokens):\n",
        "    count=0\n",
        "    for token in tokens:\n",
        "        if token in pronouns:\n",
        "            count+=1\n",
        "\n",
        "    return count\n",
        "\n",
        "#finds all characters that are not upper or lowercase letters found in english alphabet\n",
        "def uncommon_char_counter(text):\n",
        "    return len(re.findall(r'[^a-zA-Z\\s]', text))\n",
        "\n",
        "#function to count repetitions in text\n",
        "#note: I was not sure what \"average number of repetitions per post\" meant.\n",
        "# few ideas:\n",
        "# 1) repeated words, such as \"no no no\"\n",
        "# 2) letters repeating consecutively more than twice, such as \"nooooo\"\n",
        "# after inspecting the file it seems that option 2 has a lot of occurances, so I went with this.\n",
        "\n",
        "def repetition_counter(text):\n",
        "     return len(re.findall(r'(.)\\1{2,}', text))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ce650ef-cf38-41b4-9376-bde6d0de6f5a",
      "metadata": {
        "id": "9ce650ef-cf38-41b4-9376-bde6d0de6f5a"
      },
      "outputs": [],
      "source": [
        "#task 1\n",
        "#script that constructs vocabulary set for each dataframe.\n",
        "import numpy as np\n",
        "\n",
        "def vocab_set(dataframe):\n",
        "\n",
        "    #store vocabulary, using set instead of list to avoid duplicates\n",
        "    vocabulary = set()\n",
        "\n",
        "    #lengths of posts for calculating ii)\n",
        "    post_lengths = []\n",
        "\n",
        "    #pronouns for calculating iii)\n",
        "    pronoun_counts = []\n",
        "\n",
        "    #uncommon characters for calculating iv)\n",
        "    uncommon_counts = []\n",
        "\n",
        "    #repetition counts for calculating v)\n",
        "    repetition_counts = []\n",
        "\n",
        "    #iterate over the posts in the current dataframe\n",
        "    for content in dataframe[\"content\"]:\n",
        "\n",
        "        #add content tokens into vocabulary\n",
        "        tokens = tokenize(content)\n",
        "        vocabulary.update(tokens)\n",
        "\n",
        "        #add content length for ii)\n",
        "        post_lengths.append(len(tokens))\n",
        "\n",
        "        #count the pronouns found in tokens and add to list for iii)\n",
        "        pronoun_counts.append(pronoun_counter(tokens))\n",
        "\n",
        "        #count uncommon characters found in content for iv)\n",
        "        uncommon_counts.append(uncommon_char_counter(content))\n",
        "\n",
        "        #count repetitions found in content for v)\n",
        "        repetition_counts.append(repetition_counter(content))\n",
        "\n",
        "\n",
        "    stats = {\n",
        "        'Vocabulary Size': len(vocabulary),\n",
        "        'Min Length': np.min(post_lengths),\n",
        "        'Max Length': np.max(post_lengths),\n",
        "        'Avg Length': np.mean(post_lengths),\n",
        "        'Std Length': np.std(post_lengths),\n",
        "        'Avg Pronouns': np.mean(pronoun_counts),\n",
        "        'Avg Uncommon Chars': np.mean(uncommon_counts),\n",
        "        'Avg Repetitions': np.mean(repetition_counts),\n",
        "        'vocabulary': vocabulary\n",
        "    }\n",
        "\n",
        "    return stats\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d2b7f990-4a02-44c1-bfd3-9c63095aac68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b7f990-4a02-44c1-bfd3-9c63095aac68",
        "outputId": "9563b687-2c01-4c5e-d3d3-4b53e557f209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Vocabulary Size  Min Length  Max Length  Avg Length  Std Length  \\\n",
            "Category                                                                      \n",
            "sadness               10916           1          46   17.022846    8.507309   \n",
            "enthusiasm             3002           2          38   16.774704    8.314667   \n",
            "neutral               17430           1          91   14.135448    8.567085   \n",
            "worry                 16115           1          49   17.320723    8.432810   \n",
            "surprise               6657           2          57   17.229538    8.459343   \n",
            "love                   8842           1          74   16.764966    8.531608   \n",
            "fun                    6098           2          45   18.146396    8.155372   \n",
            "hate                   4629           2          54   17.725624    8.624764   \n",
            "happiness             11925           1          82   16.876752    8.421454   \n",
            "boredom                1041           3          39   16.569832    8.345165   \n",
            "relief                 4735           1          41   16.975098    8.297430   \n",
            "anger                   801           1          42   17.527273    8.695567   \n",
            "\n",
            "            Avg Pronouns  Avg Uncommon Chars  Avg Repetitions  \\\n",
            "Category                                                        \n",
            "sadness         1.360697            4.383930         0.403679   \n",
            "enthusiasm      1.279315            4.607378         0.363636   \n",
            "neutral         0.995485            4.214980         0.285367   \n",
            "worry           1.412933            4.475115         0.388344   \n",
            "surprise        1.308642            5.080933         0.409694   \n",
            "love            1.372723            4.758199         0.383134   \n",
            "fun             1.250563            5.176239         0.434685   \n",
            "hate            1.376417            4.716553         0.442933   \n",
            "happiness       1.143406            4.811480         0.389902   \n",
            "boredom         1.111732            4.402235         0.424581   \n",
            "relief          1.239843            4.354522         0.367628   \n",
            "anger           1.263636            4.681818         0.481818   \n",
            "\n",
            "                                                   vocabulary  \n",
            "Category                                                       \n",
            "sadness     {delivery, wendica, injured, poorly, shwasty, ...  \n",
            "enthusiasm  {houston, september, chiro, ot, fight, bran, c...  \n",
            "neutral     {delivery, base, kimsherrell, ot, kyleandjakie...  \n",
            "worry       {delivery, injured, poorly, kimsherrell, vs200...  \n",
            "surprise    {delivery, base, isang, certainly, 37,4, leno,...  \n",
            "love        {injured, kimsherrell, poorly, certainly, leno...  \n",
            "fun         {playground, yan, dust, carambs, certainly, le...  \n",
            "hate        {delivery, '-, lindsey5054, fight, albany, spi...  \n",
            "happiness   {delivery, base, gdgd, certainly, leno, devine...  \n",
            "boredom     {night, livebox, 11:16, possibility, you, thin...  \n",
            "relief      {houston, sight, h1n1, brisbane, ot, fight, ex...  \n",
            "anger       {you, pain, jamie_oliver, think, waynedastar, ...  \n"
          ]
        }
      ],
      "source": [
        "#make the table for task 1 and also store the vocabularies in dictionary for task 2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "summary = []\n",
        "vocabularies = {}\n",
        "\n",
        "for category, c_df in category_df.items():\n",
        "    stats = vocab_set(c_df)\n",
        "    stats['Category'] = category\n",
        "    vocabulary = stats.get('vocabulary', set())\n",
        "    vocabularies[category] = vocabulary\n",
        "    summary.append(stats)\n",
        "\n",
        "summary_df = pd.DataFrame(summary)\n",
        "summary_df.set_index('Category', inplace=True)\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c7340f94-7fa8-48f2-adf7-4a3ab8977e62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7340f94-7fa8-48f2-adf7-4a3ab8977e62",
        "outputId": "ea9e99c4-5b4c-4076-8a28-501115e8d2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             sadness  enthusiasm   neutral     worry  surprise      love  \\\n",
            "sadness     0.500000    0.122647  0.164362  0.179868  0.169806  0.169805   \n",
            "enthusiasm  0.122647    0.500000  0.092991  0.098289  0.149291  0.135427   \n",
            "neutral     0.164362    0.092991  0.500000  0.175019  0.141321  0.152253   \n",
            "worry       0.179868    0.098289  0.175019  0.500000  0.151019  0.158473   \n",
            "surprise    0.169806    0.149291  0.141321  0.151019  0.500000  0.169301   \n",
            "love        0.169805    0.135427  0.152253  0.158473  0.169301  0.500000   \n",
            "fun         0.161220    0.152527  0.134861  0.141269  0.171854  0.167604   \n",
            "hate        0.155934    0.164723  0.118727  0.131122  0.169059  0.150249   \n",
            "happiness   0.172760    0.116835  0.163073  0.171148  0.160424  0.171233   \n",
            "boredom     0.065066    0.140242  0.043203  0.047972  0.089634  0.072346   \n",
            "relief      0.157242    0.170609  0.125152  0.132758  0.174070  0.163144   \n",
            "anger       0.050952    0.113595  0.034282  0.037243  0.071869  0.056621   \n",
            "\n",
            "                 fun      hate  happiness   boredom    relief     anger  \n",
            "sadness     0.161220  0.155934   0.172760  0.065066  0.157242  0.050952  \n",
            "enthusiasm  0.152527  0.164723   0.116835  0.140242  0.170609  0.113595  \n",
            "neutral     0.134861  0.118727   0.163073  0.043203  0.125152  0.034282  \n",
            "worry       0.141269  0.131122   0.171148  0.047972  0.132758  0.037243  \n",
            "surprise    0.171854  0.169059   0.160424  0.089634  0.174070  0.071869  \n",
            "love        0.167604  0.150249   0.171233  0.072346  0.163144  0.056621  \n",
            "fun         0.500000  0.166030   0.161127  0.094691  0.177144  0.075083  \n",
            "hate        0.166030  0.500000   0.137912  0.118166  0.177381  0.095764  \n",
            "happiness   0.161127  0.137912   0.500000  0.058383  0.149160  0.045576  \n",
            "boredom     0.094691  0.118166   0.058383  0.500000  0.116170  0.166667  \n",
            "relief      0.177144  0.177381   0.149160  0.116170  0.500000  0.090860  \n",
            "anger       0.075083  0.095764   0.045576  0.166667  0.090860  0.500000  \n"
          ]
        }
      ],
      "source": [
        "#task 2\n",
        "#https://www.nltk.org/book/ch10-extras.html\n",
        "#matrix will be 13x13 or 12x12 depending on if we include the \"empty\" rows or not\n",
        "categories = list(vocabularies.keys())\n",
        "matrix = [[0 for _ in range(len(categories))] for _ in range(len(categories))]\n",
        "\n",
        "for i, category1 in enumerate(categories):\n",
        "    for j, category2 in enumerate(categories):\n",
        "\n",
        "        vocab1 = vocabularies.get(category1, set())\n",
        "        vocab2 = vocabularies.get(category2, set())\n",
        "        #intersection -> get wordds  that are in vocab1 AND 2, see nltk link\n",
        "        common_vocab = vocab1.intersection(vocab2)\n",
        "        total_size = len(vocab1) + len(vocab2)\n",
        "\n",
        "        proportion_common = len(common_vocab) / total_size\n",
        "        #store into matrix\n",
        "        matrix[i][j] = proportion_common\n",
        "\n",
        "vocab_overlap_df = pd.DataFrame(matrix, index=categories, columns=categories)\n",
        "print(vocab_overlap_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "0ea6a76d-442a-439f-bb7a-dc11126f3250",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ea6a76d-442a-439f-bb7a-dc11126f3250",
        "outputId": "d5030c60-2445-45b5-b069-1bd0f8cdfc19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             sadness  enthusiasm   neutral     worry  surprise      love  \\\n",
            "sadness     0.500000    0.450000  0.433333  0.466667  0.450000  0.383333   \n",
            "enthusiasm  0.450000    0.500000  0.466667  0.450000  0.450000  0.400000   \n",
            "neutral     0.433333    0.466667  0.500000  0.433333  0.466667  0.400000   \n",
            "worry       0.466667    0.450000  0.433333  0.500000  0.416667  0.366667   \n",
            "surprise    0.450000    0.450000  0.466667  0.416667  0.500000  0.416667   \n",
            "love        0.383333    0.400000  0.400000  0.366667  0.416667  0.500000   \n",
            "fun         0.433333    0.450000  0.450000  0.416667  0.466667  0.433333   \n",
            "hate        0.450000    0.416667  0.433333  0.433333  0.450000  0.383333   \n",
            "happiness   0.433333    0.416667  0.416667  0.400000  0.450000  0.433333   \n",
            "boredom     0.400000    0.366667  0.383333  0.400000  0.383333  0.316667   \n",
            "relief      0.416667    0.416667  0.433333  0.400000  0.433333  0.416667   \n",
            "anger       0.433333    0.400000  0.400000  0.400000  0.433333  0.400000   \n",
            "\n",
            "                 fun      hate  happiness   boredom    relief     anger  \n",
            "sadness     0.433333  0.450000   0.433333  0.400000  0.416667  0.433333  \n",
            "enthusiasm  0.450000  0.416667   0.416667  0.366667  0.416667  0.400000  \n",
            "neutral     0.450000  0.433333   0.416667  0.383333  0.433333  0.400000  \n",
            "worry       0.416667  0.433333   0.400000  0.400000  0.400000  0.400000  \n",
            "surprise    0.466667  0.450000   0.450000  0.383333  0.433333  0.433333  \n",
            "love        0.433333  0.383333   0.433333  0.316667  0.416667  0.400000  \n",
            "fun         0.500000  0.433333   0.450000  0.366667  0.433333  0.416667  \n",
            "hate        0.433333  0.500000   0.416667  0.400000  0.416667  0.416667  \n",
            "happiness   0.450000  0.416667   0.500000  0.350000  0.433333  0.433333  \n",
            "boredom     0.366667  0.400000   0.350000  0.500000  0.350000  0.383333  \n",
            "relief      0.433333  0.416667   0.433333  0.350000  0.500000  0.383333  \n",
            "anger       0.416667  0.416667   0.433333  0.383333  0.383333  0.500000  \n"
          ]
        }
      ],
      "source": [
        "#task 3, same as task 2 but for top 30 tokens instead of vocabularies\n",
        "from collections import Counter\n",
        "#function to get top 30 tokens\n",
        "def top_tokens(dataframe):\n",
        "    token_counter = Counter()\n",
        "    for content in dataframe[\"content\"]:\n",
        "        tokens = word_tokenize(content.lower())\n",
        "        token_counter.update(tokens)\n",
        "\n",
        "    top_30 = token_counter.most_common(30)\n",
        "    most_common_tokens = [item[0] for item in top_30]\n",
        "\n",
        "    return most_common_tokens\n",
        "\n",
        "#store top 30 tokens for each category\n",
        "top_30_tokens = {}\n",
        "for category, c_df in category_df.items():\n",
        "    top_30_tokens[category] = top_tokens(c_df)\n",
        "\n",
        "#same as in task 2 but modified for task 3\n",
        "categories = list(top_30_tokens.keys())\n",
        "matrix_top30 = [[0 for _ in range(len(categories))] for _ in range(len(categories))]\n",
        "\n",
        "for i, category1 in enumerate(categories):\n",
        "    for j, category2 in enumerate(categories):\n",
        "\n",
        "        tokens1 = top_30_tokens[category1]\n",
        "        tokens2 = top_30_tokens[category2]\n",
        "\n",
        "        common_tokens = list(set(tokens1) & set(tokens2))\n",
        "        total_size = 60\n",
        "\n",
        "        proportion_common_tokens = len(common_tokens) / total_size\n",
        "        matrix_top30[i][j] = proportion_common_tokens\n",
        "\n",
        "top_30_tokens_overlap_df = pd.DataFrame(matrix_top30, index=categories, columns=categories)\n",
        "print(top_30_tokens_overlap_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "25e1aac7-a8ff-488b-a740-7e20918f08a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25e1aac7-a8ff-488b-a740-7e20918f08a9",
        "outputId": "77b491de-8ab4-4102-993c-aec5157c3051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wonder', 'why', 'i', \"'m\", 'awake', 'at', '7am', ',', 'write', 'a', 'new', 'song', ',', 'plot', 'my', 'evil', 'secret', 'plot', 'muahahaha', '...', 'oh', 'damn', 'it', ',', 'not', 'secret', 'anymore']\n"
          ]
        }
      ],
      "source": [
        "# task 4\n",
        "#this section preprocesses content text. Then we use the processed text in the wnaffect script to get the emotion tags\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "sample = \"Wondering why I'm awake at 7am,writing a new song,plotting my evil secret plots muahahaha...oh damn it,not secret anymore\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "#get wordnet POS tags so that lemmatization works correctly\n",
        "#https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "#preprocess content, tokenize and lemmatize -> return lemmas, ALSO ADD STOPWORD REMOVAL!!!!\n",
        "def preprocess_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    lemmas = [lemmatizer.lemmatize(token.lower(), get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
        "    return lemmas\n",
        "\n",
        "print(preprocess_text(sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b740cc-9e2f-4344-89cb-c23859c531bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "06b740cc-9e2f-4344-89cb-c23859c531bf",
        "outputId": "9774a75e-d1e3-42cb-a810-6c7c80b65530"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed emotions added to the DataFrame and saved to 'processed_emotions_data.csv'.\n"
          ]
        }
      ],
      "source": [
        "#task 4\n",
        "#script for the WNAffect\n",
        "\n",
        "from wnaffect import WNAffect\n",
        "from emotion import Emotion # if needed\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wna = WNAffect('wordnet-1.6/', 'wn-domains-3.2/')\n",
        "\n",
        "df['processed_content'] = df['content'].apply(preprocess_text)\n",
        "#tokens = (preprocess_text(sample))\n",
        "for index, row in df.iterrows():\n",
        "    detected_emotions = []\n",
        "    tokens = row['processed_content']\n",
        "    for token in tokens:\n",
        "        emo=wna.get_emotion(token, 'JJ')\n",
        "        if emo!= None:\n",
        "            detected_emotions.append(str(emo))\n",
        "\n",
        "    df.at[index, 'emotions_detected'] = \", \".join(set(detected_emotions))\n",
        "\n",
        "df.to_csv('processed_emotions_data.csv', index=False)\n",
        "\n",
        "print(\"Processed emotions added to the DataFrame and saved to 'processed_emotions_data.csv'.\")\n",
        "\n",
        "\n",
        "#print(' -> '.join([emo.get_level(i).name for i in range(emo.level + 1)]))\n",
        "#Emotion.printTree(Emotion.emotions[\"gloom\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7427f56-b377-4512-8cd7-e3fc60f5865a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "e7427f56-b377-4512-8cd7-e3fc60f5865a",
        "outputId": "681af2c9-04aa-4bc4-f0df-4db26ceef939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 Dominant Emotions and Their Proportions:\n",
            "benevolence: 6.01%\n",
            "happiness: 3.53%\n",
            "lost-sorrow: 3.40%\n",
            "placidity: 2.43%\n",
            "eagerness: 2.08%\n",
            " benevolence┐\n",
            "            └beneficence\n"
          ]
        }
      ],
      "source": [
        "# task 4\n",
        "#get 5 most dominant emotions\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "#counter for the emotions\n",
        "emotion_counter = Counter()\n",
        "\n",
        "#emotions_detected column has the WNAffect emotions\n",
        "for emotions in df['emotions_detected']:\n",
        "    #store emotions into a list, account for commas if many emotions per cell, also ignore empty cells\n",
        "    if pd.notna(emotions) and emotions.strip():\n",
        "        emotion_list = [e.strip() for e in emotions.split(\",\")]\n",
        "        emotion_counter.update(emotion_list)\n",
        "\n",
        "#total number of emotions\n",
        "total_emotions = sum(emotion_counter.values())\n",
        "\n",
        "#proportion of each emotiom\n",
        "emotion_proportions = {emotion: count / total_emotions for emotion, count in emotion_counter.items()}\n",
        "\n",
        "top_five_emotions = Counter(emotion_proportions).most_common(5)\n",
        "\n",
        "\n",
        "#print results\n",
        "print(\"Top 5 Dominant Emotions and Their Proportions:\")\n",
        "for emotion, proportion in top_five_emotions:\n",
        "    print(f\"{emotion}: {proportion:.2%}\")\n",
        "\n",
        "Emotion.printTree(Emotion.emotions[\"benevolence\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b332918-3f4e-4a0b-8851-8a9441e8b72b",
      "metadata": {
        "id": "6b332918-3f4e-4a0b-8851-8a9441e8b72b",
        "outputId": "bffad523-2bf0-4ea2-85b2-46148da2fa87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model...\n"
          ]
        }
      ],
      "source": [
        "#task 4\n",
        "#train word2vec model using wiki-news-300d-1M.vec\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "print(\"training model...\")\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(r'C:\\Users\\Hp\\Documents\\NLP_PROJECT-20241029T133214Z-002\\NLP_PROJECT\\wiki-news-300d-1M-001.vec', binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0800ac6-e9f9-4382-a864-652096b77771",
      "metadata": {
        "id": "d0800ac6-e9f9-4382-a864-652096b77771",
        "outputId": "2fe99ecf-4c6a-428c-ddee-e2a6a04187af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.00453304 -0.01703335 -0.01173914 -0.0160204  -0.0014179  -0.010364\n",
            "  0.00784905 -0.00451288  0.0134875  -0.00104138 -0.01049903  0.0092294\n",
            "  0.00312414 -0.01275068  0.01795084 -0.00817655  0.00121392 -0.01076694\n",
            "  0.01051664  0.00709103 -0.02236621  0.00361313 -0.01501228  0.01847637\n",
            " -0.0057675   0.00245502  0.00797408  0.00245166  0.00659872 -0.00128891\n",
            "  0.01441482 -0.00157454 -0.0126182   0.00366171 -0.00367447 -0.0001765\n",
            "  0.0098573   0.01483398 -0.00198814  0.01188629  0.00811419  0.02243211\n",
            " -0.01191161  0.01007307  0.00646167 -0.00811987  0.00215277 -0.01609861\n",
            " -0.02022503 -0.00686211  0.00557647  0.00539746 -0.10291036  0.00284963\n",
            " -0.00202729 -0.0038608  -0.00522976  0.01141731 -0.00397424 -0.01166905\n",
            " -0.01178165 -0.00161453 -0.01796527 -0.0070989   0.01379756 -0.0325169\n",
            "  0.00383367 -0.00822778 -0.00388069  0.01970567 -0.00836656 -0.00049672\n",
            "  0.00704074  0.00590158 -0.01163462  0.00222528  0.01078313  0.00956004\n",
            "  0.00537248  0.03480566  0.00548286  0.02191873 -0.00587836 -0.02349059\n",
            " -0.00542697  0.01135575  0.00519186  0.0026373   0.03405656  0.01104968\n",
            "  0.01067112  0.00656583 -0.00756205  0.00236539 -0.00589485  0.00695199\n",
            "  0.01316772 -0.00937713  0.00788405 -0.00689701 -0.01918975 -0.00177793\n",
            "  0.00649542 -0.00157825  0.01070048  0.017913   -0.00270483  0.016324\n",
            "  0.01275603 -0.02198587  0.01118274 -0.01260037  0.00339896 -0.00580483\n",
            "  0.00173033 -0.01140414 -0.00438929 -0.00190884 -0.02091582 -0.03651999\n",
            " -0.00841672  0.00556518  0.00783907  0.00842106 -0.03344733  0.02099273\n",
            " -0.00129725 -0.0071788   0.01192932  0.00621008  0.00412728  0.01489511\n",
            "  0.00987069 -0.02282427  0.00385538 -0.00905141 -0.00529232  0.02387807\n",
            "  0.00377333 -0.0116159   0.00697183  0.00905907  0.0101609   0.01629801\n",
            " -0.00141314  0.02434551  0.00319609 -0.01214467 -0.0090513  -0.00733599\n",
            "  0.00372317  0.00029073 -0.01239453  0.00327862 -0.00691979  0.01010393\n",
            " -0.00478809  0.00092173  0.00654496  0.0063578  -0.01473414  0.00991926\n",
            " -0.00293874  0.02769206  0.01633879 -0.00770175  0.0083871   0.02612178\n",
            " -0.00758505 -0.00406788  0.00336565  0.0053124  -0.01897087 -0.01499112\n",
            "  0.00260709 -0.01479753  0.0259781  -0.03008148 -0.02709722  0.01038165\n",
            "  0.00684585 -0.008785   -0.00190717 -0.00363115  0.00846026  0.00218187\n",
            " -0.00677447 -0.00092181 -0.0171216   0.00926902 -0.01552608 -0.01671468\n",
            " -0.00090261 -0.0061749  -0.01499177  0.01477581 -0.0075573   0.0031459\n",
            "  0.01739524 -0.00470392 -0.00389692  0.02558745  0.00369054 -0.00819177\n",
            " -0.00329562  0.01525392  0.01361556  0.00526852  0.00420718  0.02466284\n",
            "  0.00034514 -0.00831902 -0.02069802 -0.00029212 -0.01060675  0.00996039\n",
            "  0.00135706 -0.00637977 -0.02633831 -0.00890389  0.00118423 -0.01095209\n",
            " -0.00722573  0.00497181  0.00057096  0.00670833  0.00588501  0.00542798\n",
            "  0.00286551 -0.00583462 -0.02212863 -0.00458579  0.02750449 -0.01363649\n",
            "  0.00125594 -0.00837653 -0.00512427 -0.01221831 -0.0257804  -0.0012296\n",
            " -0.01706026  0.0012601  -0.00254271 -0.00747748  0.00168314  0.00325331\n",
            "  0.00795418 -0.00075404 -0.00191659  0.04382858  0.00152148  0.01696978\n",
            " -0.00753107  0.00616468 -0.01150914 -0.00672515  0.00321641  0.0177701\n",
            "  0.01272383 -0.00514365 -0.00613528  0.00105447  0.00612537  0.00260061\n",
            " -0.05928312  0.0009267   0.00259667  0.01161208 -0.01283524  0.01885404\n",
            "  0.00573847  0.00166396  0.00938426 -0.00789409 -0.00502044 -0.00601684\n",
            "  0.00265951  0.00821158  0.01002973 -0.00124711 -0.02080609 -0.00679808\n",
            "  0.01151711  0.00240055  0.01675099  0.01803087  0.01022084 -0.00128681\n",
            "  0.00017191 -0.00933661 -0.00045915 -0.00600803  0.00575109 -0.00748776\n",
            " -0.00714547 -0.00541954  0.00245298  0.01925761 -0.00895974  0.01298568]\n"
          ]
        }
      ],
      "source": [
        "#task 4\n",
        "#weighted average word2vec vector\n",
        "\n",
        "def weighted_average_word2vec(emotions):\n",
        "    #word2vec(hate) x 0.15 + word2vec(neutral)x 0.1 + word2vec(anger) x 0.05 + word2vec(happiness) x 0.06 + word2vec(surprise) x 0.04\n",
        "    weighted_vector = 0\n",
        "    for emotion, proportion in emotions:\n",
        "        if emotion in model:\n",
        "            weighted_vector += model[emotion] * proportion\n",
        "\n",
        "    return weighted_vector\n",
        "\n",
        "weighted_avg_vector = weighted_average_word2vec(top_five_emotions)\n",
        "print(weighted_avg_vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddc1d1d3-a259-4b3f-8dd5-c56e360bbfb5",
      "metadata": {
        "id": "ddc1d1d3-a259-4b3f-8dd5-c56e360bbfb5",
        "outputId": "07395e1f-3877-4b15-d4da-bee1d452b380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity Matrix:\n",
            "                  sadness  enthusiasm  worry  surprise   fun  hate  love  happiness  relief  boredom  anger  neutral  weighted_average\n",
            "sadness             1.000       0.526  0.409     0.516 0.410 0.473 0.538      0.650   0.453    0.551  0.684    0.258             0.621\n",
            "enthusiasm          0.526       1.000  0.368     0.494 0.474 0.384 0.512      0.450   0.339    0.470  0.539    0.275             0.617\n",
            "worry               0.409       0.368  1.000     0.463 0.413 0.451 0.387      0.306   0.284    0.334  0.436    0.233             0.385\n",
            "surprise            0.516       0.494  0.463     1.000 0.457 0.414 0.425      0.414   0.454    0.372  0.497    0.280             0.455\n",
            "fun                 0.410       0.474  0.413     0.457 1.000 0.469 0.521      0.398   0.278    0.470  0.364    0.348             0.402\n",
            "hate                0.473       0.384  0.451     0.414 0.469 1.000 0.713      0.356   0.294    0.390  0.570    0.315             0.429\n",
            "love                0.538       0.512  0.387     0.425 0.521 0.713 1.000      0.541   0.314    0.435  0.460    0.270             0.572\n",
            "happiness           0.650       0.450  0.306     0.414 0.398 0.356 0.541      1.000   0.399    0.485  0.481    0.207             0.776\n",
            "relief              0.453       0.339  0.284     0.454 0.278 0.294 0.314      0.399   1.000    0.383  0.432    0.251             0.476\n",
            "boredom             0.551       0.470  0.334     0.372 0.470 0.390 0.435      0.485   0.383    1.000  0.493    0.156             0.494\n",
            "anger               0.684       0.539  0.436     0.497 0.364 0.570 0.460      0.481   0.432    0.493  1.000    0.267             0.566\n",
            "neutral             0.258       0.275  0.233     0.280 0.348 0.315 0.270      0.207   0.251    0.156  0.267    1.000             0.305\n",
            "weighted_average    0.621       0.617  0.385     0.455 0.402 0.429 0.572      0.776   0.476    0.494  0.566    0.305             1.000\n"
          ]
        }
      ],
      "source": [
        "#task 4\n",
        "#cosine similarity\n",
        "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html\n",
        "from scipy.spatial.distance import cosine\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "key_emotions = [\"sadness\",\"enthusiasm\", \"worry\", \"surprise\", \"fun\", \"hate\", \"love\", \"happiness\", \"relief\", \"boredom\", \"anger\", \"neutral\"]\n",
        "cos_similarity_matrix = np.zeros((len(key_emotions) + 1, len(key_emotions) + 1))\n",
        "\n",
        "#cosine similarity calculation\n",
        "for i, emotion in enumerate(key_emotions):\n",
        "    #first calculate cosine similarity between key_emotions\n",
        "    for j, key in enumerate(key_emotions):\n",
        "        if emotion in model and key in model:\n",
        "            cos_similarity_matrix[i, j] = 1 - cosine(model[emotion], model[key])\n",
        "\n",
        "    if emotion in model:\n",
        "        #similaritty with itself is always 1\n",
        "        cos_similarity_matrix[i, i] = 1.0\n",
        "\n",
        "        #key_emotion similarity with weighted average vector, use scipy cosine distance, see link on top for example\n",
        "        cos_similarity_matrix[i, -1] = 1 - cosine(weighted_avg_vector, model[emotion])\n",
        "        #cosine sim matrix is also symmetrical https://fr.mathworks.com/help/textanalytics/ref/cosinesimilarity.html#\n",
        "        #fills in the last row\n",
        "        cos_similarity_matrix[-1, i] = cos_similarity_matrix[i, -1]\n",
        "\n",
        "cos_similarity_matrix[-1, -1] = 1.0\n",
        "\n",
        "#print matrix\n",
        "similarity_df = pd.DataFrame(cos_similarity_matrix, index=key_emotions+ ['weighted_average'], columns=key_emotions+ ['weighted_average'])\n",
        "pd.set_option('display.float_format', '{:.3f}'.format)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "print(\"Cosine Similarity Matrix:\")\n",
        "print(similarity_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5502b2-9021-4921-a6d4-1b4a0590e3e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c5502b2-9021-4921-a6d4-1b4a0590e3e8",
        "outputId": "5d56509f-8007-4b29-e2a9-cdeff3634625"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'cp' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!cp ~/Downloads/WNAffect/* ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6789649f-6430-487e-97ed-919ae22dd368",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6789649f-6430-487e-97ed-919ae22dd368",
        "outputId": "6ce4b08e-eb4c-4961-9d9f-88bf9d72cc4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437bf18c-e819-467e-8c8e-806aed5b2a8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "437bf18c-e819-467e-8c8e-806aed5b2a8e",
        "outputId": "d2ff7953-4802-4bd2-bc01-d17f665497e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'cp' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!cp -r ~/Downloads/WNStuff/wordnet-1.6 .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "745f3cb0-b493-4537-bec8-55d9e9468b7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "745f3cb0-b493-4537-bec8-55d9e9468b7c",
        "outputId": "dbd15827-a0ac-4f49-d218-c0adba64865b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'cp' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!cp -r ~/Downloads/WNStuff/wn-domains-3.2 .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b288d43d-3bf5-4da4-af98-2b1b3f9ce5a9",
      "metadata": {
        "id": "b288d43d-3bf5-4da4-af98-2b1b3f9ce5a9",
        "outputId": "b2eb78d3-ecf8-4eac-ae4a-27da04b322ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26cfd5fc-5657-4aa7-9cc5-46e8afb6960c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "26cfd5fc-5657-4aa7-9cc5-46e8afb6960c",
        "outputId": "80b5f336-dec1-4041-ff01-5a3e8655792b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 5 Dominant Emotions from NRC Lexicon and Their Proportions:\n",
            "benevolence: 6.01%\n",
            "happiness: 3.53%\n",
            "lost-sorrow: 3.40%\n",
            "placidity: 2.43%\n",
            "eagerness: 2.08%\n",
            " benevolence┐\n",
            "            └beneficence\n",
            "\n",
            "Weighted average vector (NRC): [ 0.00453304 -0.01703335 -0.01173914 -0.0160204  -0.0014179  -0.010364\n",
            "  0.00784905 -0.00451288  0.0134875  -0.00104138 -0.01049903  0.0092294\n",
            "  0.00312414 -0.01275068  0.01795084 -0.00817655  0.00121392 -0.01076694\n",
            "  0.01051664  0.00709103 -0.02236621  0.00361313 -0.01501228  0.01847637\n",
            " -0.0057675   0.00245502  0.00797408  0.00245166  0.00659872 -0.00128891\n",
            "  0.01441482 -0.00157454 -0.0126182   0.00366171 -0.00367447 -0.0001765\n",
            "  0.0098573   0.01483398 -0.00198814  0.01188629  0.00811419  0.02243211\n",
            " -0.01191161  0.01007307  0.00646167 -0.00811987  0.00215277 -0.01609861\n",
            " -0.02022503 -0.00686211  0.00557647  0.00539746 -0.10291036  0.00284963\n",
            " -0.00202729 -0.0038608  -0.00522976  0.01141731 -0.00397424 -0.01166905\n",
            " -0.01178165 -0.00161453 -0.01796527 -0.0070989   0.01379756 -0.0325169\n",
            "  0.00383367 -0.00822778 -0.00388069  0.01970567 -0.00836656 -0.00049672\n",
            "  0.00704074  0.00590158 -0.01163462  0.00222528  0.01078313  0.00956004\n",
            "  0.00537248  0.03480567  0.00548286  0.02191873 -0.00587836 -0.02349059\n",
            " -0.00542697  0.01135575  0.00519186  0.0026373   0.03405656  0.01104968\n",
            "  0.01067112  0.00656583 -0.00756205  0.00236539 -0.00589485  0.00695199\n",
            "  0.01316772 -0.00937713  0.00788405 -0.00689701 -0.01918975 -0.00177793\n",
            "  0.00649542 -0.00157825  0.01070048  0.017913   -0.00270483  0.016324\n",
            "  0.01275603 -0.02198587  0.01118274 -0.01260037  0.00339896 -0.00580483\n",
            "  0.00173033 -0.01140414 -0.00438929 -0.00190884 -0.02091582 -0.03651999\n",
            " -0.00841672  0.00556518  0.00783907  0.00842106 -0.03344732  0.02099273\n",
            " -0.00129725 -0.0071788   0.01192932  0.00621008  0.00412728  0.01489511\n",
            "  0.00987069 -0.02282427  0.00385538 -0.00905141 -0.00529232  0.02387806\n",
            "  0.00377333 -0.0116159   0.00697183  0.00905907  0.0101609   0.01629801\n",
            " -0.00141314  0.02434551  0.00319609 -0.01214467 -0.0090513  -0.00733599\n",
            "  0.00372317  0.00029073 -0.01239453  0.00327862 -0.00691979  0.01010393\n",
            " -0.00478809  0.00092173  0.00654496  0.0063578  -0.01473414  0.00991926\n",
            " -0.00293874  0.02769206  0.01633879 -0.00770175  0.0083871   0.02612178\n",
            " -0.00758505 -0.00406788  0.00336565  0.0053124  -0.01897087 -0.01499112\n",
            "  0.00260709 -0.01479753  0.0259781  -0.03008147 -0.02709722  0.01038165\n",
            "  0.00684585 -0.008785   -0.00190717 -0.00363115  0.00846026  0.00218187\n",
            " -0.00677447 -0.00092181 -0.0171216   0.00926902 -0.01552608 -0.01671467\n",
            " -0.00090261 -0.0061749  -0.01499177  0.01477581 -0.0075573   0.0031459\n",
            "  0.01739524 -0.00470392 -0.00389692  0.02558745  0.00369054 -0.00819177\n",
            " -0.00329562  0.01525392  0.01361556  0.00526852  0.00420718  0.02466284\n",
            "  0.00034514 -0.00831902 -0.02069802 -0.00029212 -0.01060675  0.00996039\n",
            "  0.00135706 -0.00637977 -0.02633831 -0.00890389  0.00118423 -0.01095209\n",
            " -0.00722573  0.00497181  0.00057096  0.00670833  0.00588501  0.00542798\n",
            "  0.00286551 -0.00583462 -0.02212864 -0.00458579  0.02750449 -0.01363649\n",
            "  0.00125594 -0.00837653 -0.00512427 -0.01221831 -0.0257804  -0.0012296\n",
            " -0.01706026  0.0012601  -0.00254271 -0.00747748  0.00168314  0.00325331\n",
            "  0.00795418 -0.00075404 -0.00191659  0.04382858  0.00152148  0.01696978\n",
            " -0.00753107  0.00616468 -0.01150914 -0.00672515  0.00321641  0.0177701\n",
            "  0.01272383 -0.00514365 -0.00613528  0.00105447  0.00612537  0.00260061\n",
            " -0.05928312  0.0009267   0.00259667  0.01161208 -0.01283524  0.01885404\n",
            "  0.00573847  0.00166396  0.00938426 -0.00789409 -0.00502044 -0.00601684\n",
            "  0.0026595   0.00821158  0.01002972 -0.00124711 -0.02080609 -0.00679808\n",
            "  0.01151712  0.00240055  0.01675099  0.01803087  0.01022084 -0.00128681\n",
            "  0.00017191 -0.00933661 -0.00045915 -0.00600803  0.00575109 -0.00748776\n",
            " -0.00714547 -0.00541954  0.00245298  0.01925761 -0.00895974  0.01298568]\n",
            "\n",
            "Cosine Similarity Matrix (NRC):\n",
            "                  sadness  enthusiasm  worry  surprise   fun  hate  love  happiness  relief  boredom  anger  neutral  weighted_average\n",
            "sadness             1.000       0.526  0.409     0.516 0.410 0.473 0.538      0.650   0.453    0.551  0.684    0.258             0.621\n",
            "enthusiasm          0.526       1.000  0.368     0.494 0.474 0.384 0.512      0.450   0.339    0.470  0.539    0.275             0.617\n",
            "worry               0.409       0.368  1.000     0.463 0.413 0.451 0.387      0.306   0.284    0.334  0.436    0.233             0.385\n",
            "surprise            0.516       0.494  0.463     1.000 0.457 0.414 0.425      0.414   0.454    0.372  0.497    0.280             0.455\n",
            "fun                 0.410       0.474  0.413     0.457 1.000 0.469 0.521      0.398   0.278    0.470  0.364    0.348             0.402\n",
            "hate                0.473       0.384  0.451     0.414 0.469 1.000 0.713      0.356   0.294    0.390  0.570    0.315             0.429\n",
            "love                0.538       0.512  0.387     0.425 0.521 0.713 1.000      0.541   0.314    0.435  0.460    0.270             0.572\n",
            "happiness           0.650       0.450  0.306     0.414 0.398 0.356 0.541      1.000   0.399    0.485  0.481    0.207             0.776\n",
            "relief              0.453       0.339  0.284     0.454 0.278 0.294 0.314      0.399   1.000    0.383  0.432    0.251             0.476\n",
            "boredom             0.551       0.470  0.334     0.372 0.470 0.390 0.435      0.485   0.383    1.000  0.493    0.156             0.494\n",
            "anger               0.684       0.539  0.436     0.497 0.364 0.570 0.460      0.481   0.432    0.493  1.000    0.267             0.566\n",
            "neutral             0.258       0.275  0.233     0.280 0.348 0.315 0.270      0.207   0.251    0.156  0.267    1.000             0.305\n",
            "weighted_average    0.621       0.617  0.385     0.455 0.402 0.429 0.572      0.776   0.476    0.494  0.566    0.305             1.000\n"
          ]
        }
      ],
      "source": [
        "# Task 5\n",
        "# install: pip install NRCLex !\n",
        "from nrclex import NRCLex\n",
        "\n",
        "# Processing each post and identify emotions\n",
        "df['nrc_emotions_detected'] = df['content'].apply(lambda text: [emotion for emotion, score in NRCLex(text).affect_frequencies.items() if score > 0])\n",
        "\n",
        "emotion_counter = Counter()\n",
        "for emotions in df['nrc_emotions_detected']:\n",
        "    emotion_counter.update(emotions)\n",
        "\n",
        "# Total num of emotions\n",
        "total_emotions_nrc = sum(emotion_counter.values())\n",
        "\n",
        "# Proportion of each emotion\n",
        "emotion_proportions_nrc = {emotion: count / total_emotions for emotion, count in emotion_counter.items()}\n",
        "\n",
        "# Get the top 5, the top result is the empty ones so we need to get top 6\n",
        "top_five_emotions_nrc = Counter(emotion_proportions_nrc).most_common(6)\n",
        "# Delete the empty emotion\n",
        "del top_five_emotions_nrc[0]\n",
        "\n",
        "# Display the top 5 emotions\n",
        "print(\"\\nTop 5 Dominant Emotions from NRC Lexicon and Their Proportions:\")\n",
        "for emotion, proportion in top_five_emotions_nrc:\n",
        "    print(f\"{emotion}: {proportion:.2%}\")\n",
        "Emotion.printTree(Emotion.emotions[\"benevolence\"])\n",
        "\n",
        "# - Model trained in task 4 -\n",
        "\n",
        "# Weighted average word2vec vector for NRC\n",
        "def weighted_average_word2vec_NRC(emotions):\n",
        "    weighted_vector = np.zeros(model.vector_size)\n",
        "    for emotion, proportion in emotions:\n",
        "        if emotion in model:\n",
        "            weighted_vector += model[emotion] * proportion\n",
        "    return weighted_vector\n",
        "\n",
        "weighted_avg_vector_nrc = weighted_average_word2vec_NRC(top_five_emotions)\n",
        "print(\"\\nWeighted average vector (NRC):\", weighted_avg_vector_nrc)\n",
        "\n",
        "\n",
        "# - Key emotions and cosine similarity matrix defined in task 4 -\n",
        "\n",
        "# We can calculate the cosine similarities using the same code that in task 4\n",
        "for i, emotion in enumerate(key_emotions):\n",
        "    for j, key in enumerate(key_emotions):\n",
        "        if emotion in model and key in model:\n",
        "            cos_similarity_matrix[i, j] = 1 - cosine(model[emotion], model[key])\n",
        "\n",
        "    if emotion in model:\n",
        "        cos_similarity_matrix[i, i] = 1.0\n",
        "        cos_similarity_matrix[i, -1] = 1 - cosine(weighted_avg_vector_nrc, model[emotion])\n",
        "        cos_similarity_matrix[-1, i] = cos_similarity_matrix[i, -1]\n",
        "\n",
        "cos_similarity_matrix[-1, -1] = 1.0\n",
        "\n",
        "# Displaying the matrix\n",
        "similarity_df = pd.DataFrame(cos_similarity_matrix, index=key_emotions + ['weighted_average'], columns=key_emotions + ['weighted_average'])\n",
        "pd.set_option('display.float_format', '{:.3f}'.format)\n",
        "print(\"\\nCosine Similarity Matrix (NRC):\")\n",
        "print(similarity_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "phaz4THD18ms",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phaz4THD18ms",
        "outputId": "00775241-1be2-4fda-824b-d62ed945257b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training model...\n",
            "\n",
            "Circumplex Model Closeness Test Results:\n",
            "        Emotion Pair  Cosine Similarity Relation Type\n",
            "1       hate - anger              0.620         Close\n",
            "0  sadness - boredom              0.452         Close\n",
            "4    fun - happiness              0.323         Close\n",
            "3   love - happiness              0.298         Close\n",
            "6  anger - happiness              0.295       Distant\n",
            "7     boredom - love              0.174       Distant\n",
            "2         fun - love              0.156         Close\n",
            "5      sadness - fun              0.137       Distant\n"
          ]
        }
      ],
      "source": [
        "# Task 6\n",
        "from gensim.models import KeyedVectors, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Pairs for Circumplex model testing\n",
        "circumplex_pairs = {\n",
        "    'close_pairs': [(\"sadness\", \"boredom\"), (\"hate\", \"anger\"), (\"fun\", \"love\"), (\"love\", \"happiness\"), (\"fun\", \"happiness\")],\n",
        "    'distant_pairs': [(\"sadness\", \"fun\"), (\"anger\", \"happiness\"), (\"boredom\", \"love\")]  # Testing against unrelated pairs\n",
        "}\n",
        "\n",
        "# Tagging data for doc2vec training\n",
        "tagged_data = []\n",
        "for category, c_df in category_df.items():\n",
        "    for i, row in c_df.iterrows():\n",
        "        words = preprocess_text(row[\"content\"]) # Using function from task 4\n",
        "        tagged_data.append(TaggedDocument(words=words, tags=[category]))\n",
        "\n",
        "# Training the model\n",
        "print(\"training model...\")\n",
        "doc2vec_model = Doc2Vec(vector_size=300, min_count=1, epochs=30)\n",
        "doc2vec_model.build_vocab(tagged_data)\n",
        "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
        "\n",
        "# Getting vector of an emotion\n",
        "def get_emotion_vector(emotion):\n",
        "    try:\n",
        "        return doc2vec_model.dv[emotion]\n",
        "    except KeyError:\n",
        "        print(f\"Emotion '{emotion}' not found in Doc2Vec model.\")\n",
        "        return np.zeros(doc2vec_model.vector_size)\n",
        "\n",
        "# Calculating the cosine similarity\n",
        "def cosine_similarity_between_emotions(pair):\n",
        "    vec1, vec2 = get_emotion_vector(pair[0]), get_emotion_vector(pair[1])\n",
        "    return cosine_similarity([vec1], [vec2])[0, 0]\n",
        "\n",
        "# Table for results\n",
        "results = {\"Emotion Pair\": [], \"Cosine Similarity\": [], \"Relation Type\": []}\n",
        "\n",
        "for relation_type, pairs in circumplex_pairs.items():\n",
        "    for pair in pairs:\n",
        "        similarity = cosine_similarity_between_emotions(pair)\n",
        "        results[\"Emotion Pair\"].append(f\"{pair[0]} - {pair[1]}\")\n",
        "        results[\"Cosine Similarity\"].append(similarity)\n",
        "        results[\"Relation Type\"].append(\"Close\" if relation_type == \"close_pairs\" else \"Distant\")\n",
        "\n",
        "# Displaying results in a dataframe\n",
        "circumplex_df = pd.DataFrame(results)\n",
        "circumplex_df.sort_values(by=\"Cosine Similarity\", ascending=False, inplace=True)\n",
        "print(\"\\nCircumplex Model Closeness Test Results:\")\n",
        "print(circumplex_df)\n",
        "# fun-love pair is the only close pair that has lower similarity than few distant pairs, other close pairs the hypothesis is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dYXDkoi271g",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dYXDkoi271g",
        "outputId": "f0d3084f-c274-4157-aa77-47e90cc1f80d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary of Task 7 Results:\n",
            "\n",
            "Logistic Regression\n",
            "Precision: 0.34\n",
            "Recall: 0.35\n",
            "F1 Score: 0.32\n",
            "Confusion Matrix:\n",
            " [[  0   0   0   0   0   1   0   0   6   0   1   0  11]\n",
            " [  0   0   0   0   0   3   0   1   7   0   8   0  12]\n",
            " [  0   1   1   0   1  15   1   4  79   0   9   0  51]\n",
            " [  0   0   0   0   0  30   1   2  73   1  13   3  40]\n",
            " [  0   0   0   0   5  86   0  26 121   1  19   3  77]\n",
            " [  0   0   0   0  15 396   0  99 302   4  38   7 167]\n",
            " [  0   0   0   0   0   5  34   3  73   0  43   1 109]\n",
            " [  0   0   0   0   4 167   1 296 141   1  34   4 114]\n",
            " [  0   0   2   0   5 156   6  64 900   6 109  10 482]\n",
            " [  0   0   0   0   0  76   0  23 112  10  26   3 102]\n",
            " [  0   0   0   0   3  42  13  27 199   0 277   2 483]\n",
            " [  0   0   0   0   1  67   2  26 127   1  35  22 144]\n",
            " [  0   1   0   0   8 109  13  32 410   2 223   9 859]]\n",
            "\n",
            "Decision Tree\n",
            "Precision: 0.23\n",
            "Recall: 0.24\n",
            "F1 Score: 0.23\n",
            "Confusion Matrix:\n",
            " [[  0   0   0   1   1   2   0   1   5   0   1   1   7]\n",
            " [  0   0   0   0   0   5   0   0  10   4   3   1   8]\n",
            " [  0   1   6   1   4  14   6   8  50   6  23   9  34]\n",
            " [  0   1   3   5   8  30   4  12  40   4  15   9  32]\n",
            " [  0   1   4   5  22  65   7  33  71  12  26  21  71]\n",
            " [  2   1  10  18  44 240  10 150 233  31  86  49 154]\n",
            " [  0   2   6   2  11  13  33  15  53   3  52   6  72]\n",
            " [  1   2   2   5  20 134   7 220 142  26  64  29 110]\n",
            " [  3  10  27  18  45 185  36  88 643  50 200  67 368]\n",
            " [  0   4   7   2   9  50   4  32 102  20  39  13  70]\n",
            " [  1   3  10  18  29  66  36  45 203  24 234  43 334]\n",
            " [  0   1   7   6  16  59  12  37 109  16  45  18  99]\n",
            " [  2   4  27  26  52 163  62  79 385  34 274  75 483]]\n",
            "\n",
            "Naive Bayes\n",
            "Precision: 0.29\n",
            "Recall: 0.28\n",
            "F1 Score: 0.20\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    0    0    0    6    0    0    0   13]\n",
            " [   0    0    0    0    0    0    0    0    7    0    0    0   24]\n",
            " [   0    0    0    0    0    2    0    1   58    0    0    0  101]\n",
            " [   0    0    0    0    0    5    0    1   54    0    0    0  103]\n",
            " [   0    0    0    0    0   11    0    1  115    0    0    0  211]\n",
            " [   0    0    0    0    0   96    0   26  375    0    0    0  531]\n",
            " [   0    0    0    0    0    0    0    0   42    0    0    0  226]\n",
            " [   0    0    0    0    0   53    0   91  229    0    1    0  388]\n",
            " [   0    0    0    0    0   27    0   16  721    0    2    0  974]\n",
            " [   0    0    0    0    0   11    0    3  119    0    0    0  219]\n",
            " [   0    0    0    0    0    2    0    2  145    0    6    0  891]\n",
            " [   0    0    0    0    0   16    0    6  107    0    0    0  296]\n",
            " [   0    0    0    0    0    7    0    7  281    0    5    0 1366]]\n"
          ]
        }
      ],
      "source": [
        "# Task 7\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Prepare data\n",
        "X = df['content']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Splitting the training and testing data (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function for evaluating the models\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    #print(classification_report(y_test, y_pred))\n",
        "    return precision, recall, f1, conf_matrix\n",
        "\n",
        "# Tf-idf without stopword removal\n",
        "tfidf_no_stop = TfidfVectorizer(max_features=None)\n",
        "X_train_tfidf_no_stop = tfidf_no_stop.fit_transform(X_train)\n",
        "X_test_tfidf_no_stop = tfidf_no_stop.transform(X_test)\n",
        "\n",
        "\n",
        "# Machine learning models that are used\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Naive Bayes': MultinomialNB()\n",
        "}\n",
        "\n",
        "# Evaluate each model without stopwords\n",
        "results_no_stop = {}\n",
        "for name, model in models.items():\n",
        "    #print(f\"Results for {name} without stopwords:\")\n",
        "    precision, recall, f1, conf_matrix = evaluate_model(model, X_train_tfidf_no_stop, X_test_tfidf_no_stop, y_train, y_test)\n",
        "    results_no_stop[name] = {'Precision': precision, 'Recall': recall, 'F1 Score': f1, 'Confusion Matrix': conf_matrix}\n",
        "\n",
        "# Displaying results\n",
        "print(\"\\nSummary of Task 7 Results:\")\n",
        "for model_name, metrics in results_no_stop.items():\n",
        "    print(f\"\\n{model_name}\")\n",
        "    print(f\"Precision: {metrics['Precision']:.2f}\")\n",
        "    print(f\"Recall: {metrics['Recall']:.2f}\")\n",
        "    print(f\"F1 Score: {metrics['F1 Score']:.2f}\")\n",
        "    print(\"Confusion Matrix:\\n\", metrics['Confusion Matrix'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zg3LkF7V3g1x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg3LkF7V3g1x",
        "outputId": "e5904c3f-4287-4a30-c065-ff5303996bb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary of Task 8 Results:\n",
            "\n",
            "Threshold: 1000 features\n",
            "\n",
            "Logistic Regression\n",
            "Precision: 0.34\n",
            "Recall: 0.35\n",
            "F1 Score: 0.31\n",
            "Confusion Matrix:\n",
            " [[  0   0   0   0   0   1   0   0   9   0   1   0   8]\n",
            " [  0   0   0   0   0   1   0   1   8   0   7   0  14]\n",
            " [  0   0   3   0   1  11   2   6  86   0  14   1  38]\n",
            " [  0   0   0   0   0  26   2   3  78   1   8   1  44]\n",
            " [  0   0   0   0   7  91   2  26 122   1  12   7  70]\n",
            " [  0   0   0   0  21 376   2 102 318   4  30  11 164]\n",
            " [  0   0   0   0   1   8  49   2  77   0  38   1  92]\n",
            " [  0   0   0   0   6 178   2 294 152   3  33   3  91]\n",
            " [  0   0   2   0   7 150   7  56 992   6  90  10 420]\n",
            " [  0   0   0   0   2  80   0  26 135  11  18   2  78]\n",
            " [  0   0   0   0   5  51  19  33 273   2 247   6 410]\n",
            " [  0   0   0   0   1  79   3  21 150   2  25  16 128]\n",
            " [  0   0   0   0   6 110  17  46 519   2 184   7 775]]\n",
            "\n",
            "Decision Tree\n",
            "Precision: 0.24\n",
            "Recall: 0.26\n",
            "F1 Score: 0.25\n",
            "Confusion Matrix:\n",
            " [[  0   0   0   0   1   2   0   0  10   0   2   0   4]\n",
            " [  0   0   1   0   0   7   1   2   4   1   3   1  11]\n",
            " [  0   1   4   4   3  26   7   6  65   4  10   5  27]\n",
            " [  0   0   3   2   5  30   2   4  47   6  15   3  46]\n",
            " [  0   1   3   5  15  78  10  35  83   4  25  12  67]\n",
            " [  0   2   5  12  47 294  12 120 253  27  68  29 159]\n",
            " [  0   1   3   7   4  18  60   7  53   2  34  14  65]\n",
            " [  0   0   5   9  26 184   6 234 126  13  51  15  93]\n",
            " [  0   3  31  27  54 196  34  89 695  34 164  57 356]\n",
            " [  0   1   6   6  19  75   4  32  76  11  35   9  78]\n",
            " [  0   3  11   8  23  92  38  54 208  24 215  34 336]\n",
            " [  1   0   7   1   4  71  10  38 129   5  34  21 104]\n",
            " [  1   2  21  16  52 144  56  84 411  33 250  62 534]]\n",
            "\n",
            "Naive Bayes\n",
            "Precision: 0.32\n",
            "Recall: 0.32\n",
            "F1 Score: 0.28\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    1    0    0    9    0    0    0    9]\n",
            " [   0    0    0    0    0    0    0    1   10    0    5    0   15]\n",
            " [   0    0    0    0    0    9    0    5   91    0    9    0   48]\n",
            " [   0    0    0    0    0   21    0    3   85    0    3    0   51]\n",
            " [   0    0    0    0    0   71    0   18  141    0    2    0  106]\n",
            " [   0    0    0    0    0  305    0   81  426    0   12    1  203]\n",
            " [   0    0    0    0    0    8   12    1   81    0   28    0  138]\n",
            " [   0    0    0    0    0  156    0  237  219    0   22    1  127]\n",
            " [   0    0    0    0    0  113    1   46 1010    0   47    1  522]\n",
            " [   0    0    0    0    0   55    0   20  171    0    5    0  101]\n",
            " [   0    0    0    0    0   38    3   21  307    0  145    0  532]\n",
            " [   0    0    0    0    0   58    1   16  176    0   10    5  159]\n",
            " [   0    0    0    0    0   72    2   33  586    0   89    1  883]]\n",
            "\n",
            "Threshold: 500 features\n",
            "\n",
            "Logistic Regression\n",
            "Precision: 0.33\n",
            "Recall: 0.34\n",
            "F1 Score: 0.30\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    1    0    0    0    8    0    0    0   10]\n",
            " [   0    0    0    0    0    1    0    1   12    0    5    0   12]\n",
            " [   0    0    3    0    1   12    1    7   82    0   12    1   43]\n",
            " [   0    0    0    0    0   27    2    3   80    1    9    1   40]\n",
            " [   0    0    0    0    7   85    3   25  121    0   14    5   78]\n",
            " [   0    0    0    0   22  367    2   93  319    3   24   10  188]\n",
            " [   0    0    0    0    1    6   47    2   81    0   34    1   96]\n",
            " [   0    0    0    0    8  175    1  272  170    2   28    5  101]\n",
            " [   0    0    2    0   11  151    7   55 1015    5   74    7  413]\n",
            " [   0    0    1    0    1   85    0   22  131    9   15    2   86]\n",
            " [   0    1    1    0    8   51   18   34  266    1  214    6  446]\n",
            " [   0    0    0    0    1   79    3   20  159    1   22   16  124]\n",
            " [   0    0    0    0   10  125   22   38  549    1  168    6  747]]\n",
            "\n",
            "Decision Tree\n",
            "Precision: 0.24\n",
            "Recall: 0.26\n",
            "F1 Score: 0.25\n",
            "Confusion Matrix:\n",
            " [[  0   0   0   0   2   1   0   1   9   1   0   0   5]\n",
            " [  0   1   2   0   0   3   0   3   8   1   4   2   7]\n",
            " [  0   1   4   2   2  16  10  12  68   2  14   3  28]\n",
            " [  0   0   2   3   5  32   1   7  53   3  15   5  37]\n",
            " [  0   1   1   7  20  74   3  31  83  12  32  17  57]\n",
            " [  0   1   5  14  55 275  20 129 266  34  69  38 122]\n",
            " [  0   0   3   2   3  14  50   5  70   1  45  11  64]\n",
            " [  0   0   3   5  25 179   6 222 150  21  49  20  82]\n",
            " [  0   5  25  17  41 195  25  84 788  39 164  50 307]\n",
            " [  1   3   5   5   7  73   4  36  95  15  35   9  64]\n",
            " [  1   2  14   6  23  72  38  54 237  22 232  38 307]\n",
            " [  1   1   6   2  10  66   4  37 129   7  36  21 105]\n",
            " [  3   4  23  11  38 163  55  86 472  33 259  56 463]]\n",
            "\n",
            "Naive Bayes\n",
            "Precision: 0.31\n",
            "Recall: 0.32\n",
            "F1 Score: 0.27\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    1    0    0    9    0    0    0    9]\n",
            " [   0    0    0    0    0    0    0    1   12    0    5    0   13]\n",
            " [   0    0    0    0    0   10    0    4   87    0    7    0   54]\n",
            " [   0    0    0    0    0   24    0    3   84    0    5    0   47]\n",
            " [   0    0    0    0    0   65    1   19  151    0    3    0   99]\n",
            " [   0    0    0    0    0  286    0   75  425    0   16    1  225]\n",
            " [   0    0    0    0    0    7   15    3   86    0   26    0  131]\n",
            " [   0    0    0    0    0  143    0  218  241    0   15    1  144]\n",
            " [   0    0    0    0    0  120    1   51 1030    0   36    2  500]\n",
            " [   0    0    0    0    0   53    0   20  163    0    4    0  112]\n",
            " [   0    0    0    0    0   39    3   23  309    0  126    0  546]\n",
            " [   0    0    0    0    0   60    1   18  180    0   10    9  147]\n",
            " [   0    0    0    0    1   75    7   33  602    0   89    2  857]]\n",
            "\n",
            "Threshold: 100 features\n",
            "\n",
            "Logistic Regression\n",
            "Precision: 0.32\n",
            "Recall: 0.31\n",
            "F1 Score: 0.27\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    1    0    0   10    0    1    0    7]\n",
            " [   0    0    0    0    0    2    0    1   19    0    2    0    7]\n",
            " [   0    0    0    0    0   16    1    5   96    0    3    0   41]\n",
            " [   0    0    0    0    0   33    0    2   79    0    4    0   45]\n",
            " [   0    0    0    0    1   89    2   18  144    0    4    0   80]\n",
            " [   0    0    0    0    6  365    1   81  355    0   23    0  197]\n",
            " [   0    0    0    0    0   13   35    2  112    0   15    0   91]\n",
            " [   0    0    0    0    2  152    2  254  221    0   30    0  101]\n",
            " [   0    0    0    0    0  170    4   55 1063    0   54    0  394]\n",
            " [   0    0    0    0    0   71    0   19  162    1   12    0   87]\n",
            " [   0    0    0    0    2   63    9   30  383    0  141    0  418]\n",
            " [   0    0    0    0    0   65    2   22  216    0    8    0  112]\n",
            " [   0    0    0    0    3  143   16   36  754    0   98    0  616]]\n",
            "\n",
            "Decision Tree\n",
            "Precision: 0.24\n",
            "Recall: 0.26\n",
            "F1 Score: 0.24\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    2    0    1    1   11    0    2    1    1]\n",
            " [   0    0    0    2    3    2    0    1   13    0    3    1    6]\n",
            " [   0    0    2    1    4   24    3    6   84    3   12    2   21]\n",
            " [   0    0    1    3    5   34    4   10   72    4    8    1   21]\n",
            " [   1    0    4    4   21   66   10   19  123    5   29    6   50]\n",
            " [   0    3   10    8   45  265    7  118  352   21   64   22  113]\n",
            " [   0    0    3    4    9   14   32    9  117    4   27    1   48]\n",
            " [   2    1    4    5   22  137    4  224  223   15   51   14   60]\n",
            " [   5    2    6   20   31  180   27   82 1014   24  121   24  204]\n",
            " [   1    1    2    4   11   58    5   30  142   10   23    8   57]\n",
            " [   1    3   12    8   16   72   23   46  405   14  184   21  241]\n",
            " [   0    0    4    1    8   59    5   30  208    5   29   15   61]\n",
            " [   2    1   14   18   45  170   28   72  730   22  200   29  335]]\n",
            "\n",
            "Naive Bayes\n",
            "Precision: 0.27\n",
            "Recall: 0.29\n",
            "F1 Score: 0.24\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    1    0    0   10    0    0    0    8]\n",
            " [   0    0    0    0    0    0    0    4   20    0    1    0    6]\n",
            " [   0    0    0    0    0   10    0    7  101    0    1    0   43]\n",
            " [   0    0    0    0    0   25    0    4   87    0    1    0   46]\n",
            " [   0    0    0    0    0   54    2   14  185    0    1    0   82]\n",
            " [   0    0    0    0    1  232    1  102  468    0    9    0  215]\n",
            " [   0    0    0    0    0    6   23    2  132    0   11    0   94]\n",
            " [   0    0    0    0    0  102    1  225  281    0   19    0  134]\n",
            " [   0    0    0    0    0  111    2   60 1132    0   29    0  406]\n",
            " [   0    0    0    0    0   44    0   15  200    0    6    0   87]\n",
            " [   0    0    0    0    0   34    8   31  436    0   99    0  438]\n",
            " [   0    0    0    0    0   44    1   21  241    0    1    0  117]\n",
            " [   0    0    0    0    0   93   13   41  814    0   61    0  644]]\n"
          ]
        }
      ],
      "source": [
        "# Task 8\n",
        "\n",
        "# Different feature thresholds\n",
        "feature_thresholds = [1000, 500, 100]\n",
        "\n",
        "# Stopwords removal and evaluation at different thresholds\n",
        "results_with_stop = {}\n",
        "for threshold in feature_thresholds:\n",
        "    #print(f\"\\nEvaluating models with stopword removal and max_features={threshold}\\n\")\n",
        "    tfidf_with_stop = TfidfVectorizer(stop_words='english', max_features=threshold)\n",
        "    X_train_tfidf_with_stop = tfidf_with_stop.fit_transform(X_train)\n",
        "    X_test_tfidf_with_stop = tfidf_with_stop.transform(X_test)\n",
        "\n",
        "    threshold_results = {}\n",
        "    for name, model in models.items():\n",
        "        #print(f\"Results for {name} with stopwords removed and max_features={threshold}:\")\n",
        "        precision, recall, f1, conf_matrix = evaluate_model(model, X_train_tfidf_with_stop, X_test_tfidf_with_stop, y_train, y_test) # function defined in task 7\n",
        "        threshold_results[name] = {'Precision': precision, 'Recall': recall, 'F1 Score': f1, 'Confusion Matrix': conf_matrix}\n",
        "\n",
        "    results_with_stop[threshold] = threshold_results\n",
        "\n",
        "# Displaying results\n",
        "print(\"\\nSummary of Task 8 Results:\")\n",
        "for threshold, models_results in results_with_stop.items():\n",
        "    print(f\"\\nThreshold: {threshold} features\")\n",
        "    for model_name, metrics in models_results.items():\n",
        "        print(f\"\\n{model_name}\")\n",
        "        print(f\"Precision: {metrics['Precision']:.2f}\")\n",
        "        print(f\"Recall: {metrics['Recall']:.2f}\")\n",
        "        print(f\"F1 Score: {metrics['F1 Score']:.2f}\")\n",
        "        print(\"Confusion Matrix:\\n\", metrics['Confusion Matrix'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0209f453",
      "metadata": {
        "id": "0209f453",
        "outputId": "1f9668b1-8027-4530-f51c-14237c348db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary of Task 9 Results with Bigrams:\n",
            "\n",
            "Threshold: 1000 features (Bigrams)\n",
            "\n",
            "Logistic Regression\n",
            "Precision: 0.29\n",
            "Recall: 0.26\n",
            "F1 Score: 0.19\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    0    0    0   16    0    0    0    3]\n",
            " [   0    0    0    0    0    0    0    0   27    0    0    0    4]\n",
            " [   0    0    0    0    0    8    0    4  136    0    3    1   10]\n",
            " [   0    0    0    0    1   16    0    4  126    0    4    0   12]\n",
            " [   0    0    0    0    0   36    0    7  249    0   13    0   33]\n",
            " [   0    0    0    0    3  160    0   68  699    2   16    0   80]\n",
            " [   0    0    0    0    0    5    2    2  205    0   16    1   37]\n",
            " [   0    0    0    0    0   77    0  125  495    1    5    1   58]\n",
            " [   0    0    0    0    0   65    1   32 1477    0   24    0  141]\n",
            " [   0    0    0    0    0   29    0   11  251    9    9    0   43]\n",
            " [   0    0    0    0    0   31    0   12  777    3   46    1  176]\n",
            " [   0    0    0    0    0   22    0   12  342    0    8    2   39]\n",
            " [   0    0    0    0    0   71    0   22 1232    1   49    4  287]]\n",
            "\n",
            "Decision Tree\n",
            "Precision: 0.24\n",
            "Recall: 0.25\n",
            "F1 Score: 0.18\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    1    0    0    1   15    0    1    0    1]\n",
            " [   0    0    0    0    0    1    1    1   26    0    0    0    2]\n",
            " [   0    1    0    1    2   13    0    4  126    1    5    3    6]\n",
            " [   0    0    0    1    0   17    1    5  128    1    3    0    7]\n",
            " [   0    1    0    1    5   39    0    8  250    1    6    0   27]\n",
            " [   0    0    1    6   21  155    3   77  676    4   22    3   60]\n",
            " [   0    0    0    2    0   11    4    2  202    0   13    4   30]\n",
            " [   0    0    1    3   10   81    1  121  482    3   11    5   44]\n",
            " [   0    3    4    3   18   89    3   42 1445    2   40    4   87]\n",
            " [   0    2    2    5    4   32    0   20  241    8   12    2   24]\n",
            " [   0    0    2    2   10   52    2   21  770    4   68    4  111]\n",
            " [   0    1    0    0    2   33    1   15  336    0   11    3   23]\n",
            " [   0    1    5    5   18   76    9   46 1220    8   69    9  200]]\n",
            "\n",
            "Naive Bayes\n",
            "Precision: 0.33\n",
            "Recall: 0.26\n",
            "F1 Score: 0.18\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    0    0    0   16    0    0    0    3]\n",
            " [   0    0    0    0    0    0    0    0   27    0    1    0    3]\n",
            " [   0    0    0    0    0    4    0    3  141    0    3    0   11]\n",
            " [   0    0    0    0    0   11    0    3  132    0    4    0   13]\n",
            " [   0    0    0    0    0   27    0    5  265    0   10    0   31]\n",
            " [   0    0    0    0    0  112    0   56  764    0   13    0   83]\n",
            " [   0    0    0    0    0    4    2    0  210    0   11    0   41]\n",
            " [   0    0    0    0    0   50    0  114  531    1    3    0   63]\n",
            " [   0    0    0    0    0   39    0   29 1522    0   19    0  131]\n",
            " [   0    0    0    0    0   18    0    8  271    2   10    0   43]\n",
            " [   0    0    0    0    0   14    0    8  802    0   43    0  179]\n",
            " [   0    0    0    0    0   16    0   11  355    0    6    1   36]\n",
            " [   0    0    0    0    0   41    0   15 1281    0   42    1  286]]\n",
            "\n",
            "Threshold: 500 features (Bigrams)\n",
            "\n",
            "Logistic Regression\n",
            "Precision: 0.27\n",
            "Recall: 0.26\n",
            "F1 Score: 0.18\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    0    0    0   17    0    0    0    2]\n",
            " [   0    0    0    0    0    0    0    0   28    0    0    0    3]\n",
            " [   0    0    0    0    0    6    0    3  143    0    1    1    8]\n",
            " [   0    0    0    0    1   12    0    4  132    0    2    0   12]\n",
            " [   0    0    0    0    0   31    0    3  266    0   11    0   27]\n",
            " [   0    0    0    0    2  142    0   58  735    2   12    0   77]\n",
            " [   0    0    0    0    0    5    0    1  220    0    9    1   32]\n",
            " [   0    0    0    0    0   67    0  117  519    0    4    1   54]\n",
            " [   0    0    0    0    0   58    0   27 1520    2   16    0  117]\n",
            " [   0    0    0    0    0   20    0    9  270   10    5    1   37]\n",
            " [   0    0    0    0    0   24    0    8  806    4   43    1  160]\n",
            " [   0    0    0    0    0   15    0   13  361    0    7    3   26]\n",
            " [   0    0    0    0    0   57    0   17 1301    1   39    5  246]]\n",
            "\n",
            "Decision Tree\n",
            "Precision: 0.25\n",
            "Recall: 0.25\n",
            "F1 Score: 0.18\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    1    0    0    0   15    0    0    0    3]\n",
            " [   0    0    0    0    0    0    0    0   29    0    0    0    2]\n",
            " [   0    0    0    0    0   11    0    4  136    0    3    1    7]\n",
            " [   0    0    0    0    1   13    0    6  130    1    1    1   10]\n",
            " [   0    1    0    0    2   33    0    7  266    1    6    0   22]\n",
            " [   0    0    1    1   12  142    1   67  721    3   15    4   61]\n",
            " [   0    0    0    0    0    8    2    3  221    1    4    1   28]\n",
            " [   0    0    3    0    5   67    2  119  510    3   10    4   39]\n",
            " [   0    1    0    1   10   74    2   40 1502    1   20    6   83]\n",
            " [   0    1    0    1    1   20    0   18  272    7    3    2   27]\n",
            " [   0    1    0    3    3   42    1   18  812    3   45    4  114]\n",
            " [   0    1    0    0    3   23    0   14  356    0    6    3   19]\n",
            " [   0    1    1    1    6   65    5   35 1297    2   44    9  200]]\n",
            "\n",
            "Naive Bayes\n",
            "Precision: 0.27\n",
            "Recall: 0.26\n",
            "F1 Score: 0.18\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    0    0    0   17    0    0    0    2]\n",
            " [   0    0    0    0    0    0    0    0   29    0    0    0    2]\n",
            " [   0    0    0    0    0    6    0    3  145    0    1    0    7]\n",
            " [   0    0    0    0    0   14    0    2  134    0    2    0   11]\n",
            " [   0    0    0    0    0   23    0    2  280    0    7    0   26]\n",
            " [   0    0    0    0    3  119    0   51  771    2   12    0   70]\n",
            " [   0    0    0    0    0    3    0    0  224    0    6    0   35]\n",
            " [   0    0    0    0    0   52    0  112  545    0    2    0   51]\n",
            " [   0    0    0    0    1   43    0   25 1557    0   17    0   97]\n",
            " [   0    0    0    0    0   10    0    9  291    5    4    0   33]\n",
            " [   0    0    0    0    1   16    0    6  832    1   43    0  147]\n",
            " [   0    0    0    0    0   10    0   11  373    0    6    0   25]\n",
            " [   0    0    0    0    1   42    0   12 1339    1   36    1  234]]\n",
            "\n",
            "Threshold: 100 features (Bigrams)\n",
            "\n",
            "Logistic Regression\n",
            "Precision: 0.25\n",
            "Recall: 0.24\n",
            "F1 Score: 0.15\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    0    0    0   18    0    0    0    1]\n",
            " [   0    0    0    0    0    1    0    0   28    0    1    0    1]\n",
            " [   0    0    0    0    0    4    0    1  153    0    1    0    3]\n",
            " [   0    0    0    0    0    8    0    2  145    0    1    0    7]\n",
            " [   0    0    0    0    0   17    0    1  299    0    3    0   18]\n",
            " [   0    0    0    0    0   92    0   45  852    2    4    0   33]\n",
            " [   0    0    0    0    0    4    0    0  243    0    2    0   19]\n",
            " [   0    0    0    0    0   38    0   94  593    0    3    0   34]\n",
            " [   0    0    0    0    0   48    0   22 1606    0   10    0   54]\n",
            " [   0    0    0    0    0   13    0    9  298    1    3    0   28]\n",
            " [   0    0    0    0    0   20    0    4  934    1   18    0   69]\n",
            " [   0    0    0    0    0   11    0   10  390    0    2    0   12]\n",
            " [   0    0    0    0    0   36    0    9 1483    1   14    0  123]]\n",
            "\n",
            "Decision Tree\n",
            "Precision: 0.26\n",
            "Recall: 0.24\n",
            "F1 Score: 0.15\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    1    0    0    0   17    0    0    0    1]\n",
            " [   0    0    0    0    0    1    0    0   28    0    1    0    1]\n",
            " [   0    0    0    1    0    3    0    2  151    0    2    0    3]\n",
            " [   0    0    0    0    0   10    0    2  145    0    0    0    6]\n",
            " [   0    0    0    0    0   19    0    2  299    1    6    0   11]\n",
            " [   0    0    0    0    1  101    0   43  842    4    7    0   30]\n",
            " [   0    0    0    0    0    6    0    0  244    0    5    0   13]\n",
            " [   0    0    0    1    0   36    0   93  595    1    7    2   27]\n",
            " [   0    0    0    0    2   52    0   20 1602    0   14    0   50]\n",
            " [   0    0    0    0    0   14    1    8  300    1    6    0   22]\n",
            " [   0    0    0    0    0   24    0    7  930    1   23    0   61]\n",
            " [   0    0    0    0    0   12    0    9  391    1    3    1    8]\n",
            " [   0    0    0    1    0   36    2   10 1484    1   22    0  110]]\n",
            "\n",
            "Naive Bayes\n",
            "Precision: 0.24\n",
            "Recall: 0.24\n",
            "F1 Score: 0.14\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    0    0    0   18    0    0    0    1]\n",
            " [   0    0    0    0    0    1    0    0   28    0    1    0    1]\n",
            " [   0    0    0    0    0    3    0    1  154    0    1    0    3]\n",
            " [   0    0    0    0    0    7    0    2  145    0    1    0    8]\n",
            " [   0    0    0    0    0   14    0    1  298    0    3    0   22]\n",
            " [   0    0    0    0    0   70    0   41  865    2    6    0   44]\n",
            " [   0    0    0    0    0    1    0    0  244    0    3    0   20]\n",
            " [   0    0    0    0    0   29    0   90  598    0    3    0   42]\n",
            " [   0    0    0    0    0   30    0   22 1613    0   13    0   62]\n",
            " [   0    0    0    0    0    5    0    8  305    1    3    0   30]\n",
            " [   0    0    0    0    0    9    0    4  939    1   20    0   73]\n",
            " [   0    0    0    0    0    8    0    8  393    0    3    0   13]\n",
            " [   0    0    0    0    0   21    0    9 1490    1   17    0  128]]\n"
          ]
        }
      ],
      "source": [
        "#Task 9: Task 8 with Bigrams\n",
        "\n",
        "# Different feature thresholds\n",
        "feature_thresholds = [1000, 500, 100]\n",
        "\n",
        "# Stopwords removal and evaluation at different thresholds using bigrams\n",
        "results_with_stop_bigrams = {}\n",
        "for threshold in feature_thresholds:\n",
        "    #print(f\"\\nEvaluating models with stopword removal, bigrams, and max_features={threshold}\\n\")\n",
        "    tfidf_with_stop_bigrams = TfidfVectorizer(stop_words='english', max_features=threshold, ngram_range=(2, 2))\n",
        "    X_train_tfidf_with_stop_bigrams = tfidf_with_stop_bigrams.fit_transform(X_train)\n",
        "    X_test_tfidf_with_stop_bigrams = tfidf_with_stop_bigrams.transform(X_test)\n",
        "\n",
        "    threshold_results_bigrams = {}\n",
        "    for name, model in models.items():\n",
        "        #print(f\"Results for {name} with stopwords removed, bigrams, and max_features={threshold}:\")\n",
        "        precision, recall, f1, conf_matrix = evaluate_model(model, X_train_tfidf_with_stop_bigrams, X_test_tfidf_with_stop_bigrams, y_train, y_test) # function defined in task 7\n",
        "        threshold_results_bigrams[name] = {'Precision': precision, 'Recall': recall, 'F1 Score': f1, 'Confusion Matrix': conf_matrix}\n",
        "\n",
        "    results_with_stop_bigrams[threshold] = threshold_results_bigrams\n",
        "\n",
        "# Displaying results\n",
        "print(\"\\nSummary of Task 9 Results with Bigrams:\")\n",
        "for threshold, models_results in results_with_stop_bigrams.items():\n",
        "    print(f\"\\nThreshold: {threshold} features (Bigrams)\")\n",
        "    for model_name, metrics in models_results.items():\n",
        "        print(f\"\\n{model_name}\")\n",
        "        print(f\"Precision: {metrics['Precision']:.2f}\")\n",
        "        print(f\"Recall: {metrics['Recall']:.2f}\")\n",
        "        print(f\"F1 Score: {metrics['F1 Score']:.2f}\")\n",
        "        print(\"Confusion Matrix:\\n\", metrics['Confusion Matrix'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ba33fe",
      "metadata": {
        "id": "25ba33fe",
        "outputId": "91fe0f5d-314a-4767-877c-112b307bc2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Architecture:\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 100, 100)          1000000   \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 100, 128)          38528     \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 100, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 100, 128)          0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 100, 128)          65664     \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 100, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 100, 128)          0         \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 100, 128)          82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_4 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 13)                1677      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,256,397\n",
            "Trainable params: 1,255,117\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "800/800 [==============================] - 43s 52ms/step - loss: 2.4463 - accuracy: 0.2049 - val_loss: 2.0880 - val_accuracy: 0.2580\n",
            "Epoch 2/15\n",
            "800/800 [==============================] - 46s 58ms/step - loss: 2.0350 - accuracy: 0.3080 - val_loss: 1.9488 - val_accuracy: 0.3222\n",
            "Epoch 3/15\n",
            "800/800 [==============================] - 48s 60ms/step - loss: 1.8907 - accuracy: 0.3591 - val_loss: 1.9596 - val_accuracy: 0.3372\n",
            "Epoch 4/15\n",
            "800/800 [==============================] - 46s 57ms/step - loss: 1.7927 - accuracy: 0.3945 - val_loss: 1.9564 - val_accuracy: 0.3375\n",
            "Epoch 5/15\n",
            "800/800 [==============================] - 46s 57ms/step - loss: 1.6943 - accuracy: 0.4268 - val_loss: 2.0066 - val_accuracy: 0.3352\n",
            "250/250 [==============================] - 4s 14ms/step\n",
            "\n",
            "Per-class Performance:\n",
            "\n",
            "Class: anger\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Support: 19\n",
            "\n",
            "Class: boredom\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Support: 31\n",
            "\n",
            "Class: empty\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Support: 162\n",
            "\n",
            "Class: enthusiasm\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Support: 163\n",
            "\n",
            "Class: fun\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Support: 338\n",
            "\n",
            "Class: happiness\n",
            "Precision: 0.32\n",
            "Recall: 0.31\n",
            "F1 Score: 0.32\n",
            "Support: 1028\n",
            "\n",
            "Class: hate\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Support: 268\n",
            "\n",
            "Class: love\n",
            "Precision: 0.53\n",
            "Recall: 0.30\n",
            "F1 Score: 0.38\n",
            "Support: 762\n",
            "\n",
            "Class: neutral\n",
            "Precision: 0.33\n",
            "Recall: 0.67\n",
            "F1 Score: 0.44\n",
            "Support: 1740\n",
            "\n",
            "Class: relief\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Support: 352\n",
            "\n",
            "Class: sadness\n",
            "Precision: 0.30\n",
            "Recall: 0.08\n",
            "F1 Score: 0.12\n",
            "Support: 1046\n",
            "\n",
            "Class: surprise\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Support: 425\n",
            "\n",
            "Class: worry\n",
            "Precision: 0.31\n",
            "Recall: 0.51\n",
            "F1 Score: 0.38\n",
            "Support: 1666\n",
            "\n",
            "Overall CNN Model Results:\n",
            "Precision: 0.27\n",
            "Recall: 0.33\n",
            "F1 Score: 0.27\n",
            "Confusion Matrix:\n",
            " [[   0    0    0    0    0    0    0    0   10    0    2    0    7]\n",
            " [   0    0    0    0    0    2    0    0   11    0    2    0   16]\n",
            " [   0    0    0    0    0    6    0    4  101    0    6    0   45]\n",
            " [   0    0    0    0    0   19    0    3   97    0    4    0   40]\n",
            " [   0    0    0    0    0   85    0   17  155    0    0    0   81]\n",
            " [   0    0    0    0    0  322    0   76  424    0    2    0  204]\n",
            " [   0    0    0    0    0    6    0    0   78    0   17    0  167]\n",
            " [   0    0    0    0    0  184    0  230  206    0    3    0  139]\n",
            " [   0    0    0    0    0  136    0   31 1158    0   39    0  376]\n",
            " [   0    0    0    0    0   65    0   16  162    0    5    0  104]\n",
            " [   0    0    0    0    0   24    0   16  342    0   80    0  584]\n",
            " [   0    0    0    0    0   61    0   19  202    0    8    0  135]\n",
            " [   0    0    0    0    0   83    0   22  614    0  103    0  844]]\n"
          ]
        }
      ],
      "source": [
        "#Task 10\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class EmotionCNNClassifier:\n",
        "    def __init__(self, max_words=10000, max_len=100, embedding_dim=100):\n",
        "        self.max_words = max_words\n",
        "        self.max_len = max_len\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.tokenizer = None\n",
        "        self.label_encoder = None\n",
        "        self.model = None\n",
        "\n",
        "    def preprocess_data(self, X_train, X_test, y_train, y_test):\n",
        "        \"\"\"Preprocess text data and labels for CNN.\"\"\"\n",
        "        # Convert to numpy arrays if needed\n",
        "        if hasattr(X_train, 'values'):\n",
        "            X_train = X_train.values\n",
        "        if hasattr(X_test, 'values'):\n",
        "            X_test = X_test.values\n",
        "        if hasattr(y_train, 'values'):\n",
        "            y_train = y_train.values\n",
        "        if hasattr(y_test, 'values'):\n",
        "            y_test = y_test.values\n",
        "\n",
        "        # Process text data\n",
        "        self.tokenizer = Tokenizer(num_words=self.max_words)\n",
        "        self.tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "        X_train_seq = self.tokenizer.texts_to_sequences(X_train)\n",
        "        X_test_seq = self.tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "        X_train_pad = pad_sequences(X_train_seq, maxlen=self.max_len)\n",
        "        X_test_pad = pad_sequences(X_test_seq, maxlen=self.max_len)\n",
        "\n",
        "        # Process labels\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        y_train_encoded = self.label_encoder.fit_transform(y_train)\n",
        "        y_test_encoded = self.label_encoder.transform(y_test)\n",
        "\n",
        "        return X_train_pad, X_test_pad, y_train_encoded, y_test_encoded\n",
        "\n",
        "    def create_model(self, num_classes):\n",
        "        \"\"\"Create CNN model with improved architecture.\"\"\"\n",
        "        vocab_size = min(self.max_words, len(self.tokenizer.word_index) + 1)\n",
        "\n",
        "        model = Sequential([\n",
        "            # Embedding layer\n",
        "            Embedding(vocab_size, self.embedding_dim, input_length=self.max_len),\n",
        "\n",
        "            # Multiple Conv1D layers with different kernel sizes\n",
        "            Conv1D(128, 3, activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Conv1D(128, 4, activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Conv1D(128, 5, activation='relu', padding='same'),\n",
        "            GlobalMaxPooling1D(),\n",
        "\n",
        "            # Dense layers with dropout and batch normalization\n",
        "            Dense(256, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "\n",
        "            Dense(128, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        optimizer = Adam(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer,\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, all_labels):\n",
        "    return {\n",
        "        'precision': precision_score(y_true, y_pred,\n",
        "                                   average='weighted',\n",
        "                                   zero_division=0,\n",
        "                                   labels=all_labels),\n",
        "        'recall': recall_score(y_true, y_pred,\n",
        "                             average='weighted',\n",
        "                             zero_division=0,\n",
        "                             labels=all_labels),\n",
        "        'f1': f1_score(y_true, y_pred,\n",
        "                      average='weighted',\n",
        "                      zero_division=0,\n",
        "                      labels=all_labels),\n",
        "        'confusion_matrix': confusion_matrix(y_true, y_pred,\n",
        "                                           labels=all_labels)\n",
        "    }\n",
        "\n",
        "def train_and_evaluate_cnn(X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Train and evaluate CNN model with proper preprocessing and metric calculation.\"\"\"\n",
        "    # Initialize classifier\n",
        "    classifier = EmotionCNNClassifier(max_words=10000, max_len=100, embedding_dim=100)\n",
        "\n",
        "    # Get unique labels before encoding\n",
        "    all_labels = np.unique(np.concatenate([y_train, y_test]))\n",
        "\n",
        "    # Preprocess data\n",
        "    X_train_pad, X_test_pad, y_train_encoded, y_test_encoded = classifier.preprocess_data(\n",
        "        X_train, X_test, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    num_classes = len(all_labels)\n",
        "    classifier.create_model(num_classes)\n",
        "\n",
        "    # Print model summary\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    classifier.model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=3,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            'best_emotion_model.h5',\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    history = classifier.model.fit(\n",
        "        X_train_pad, y_train_encoded,\n",
        "        epochs=15,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred_probs = classifier.model.predict(X_test_pad)\n",
        "    y_pred_encoded = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Convert predictions back to original labels\n",
        "    y_pred = classifier.label_encoder.inverse_transform(y_pred_encoded)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    overall_metrics = calculate_metrics(y_test, y_pred, all_labels)\n",
        "\n",
        "    # Calculate per-class metrics\n",
        "    print(\"\\nPer-class Performance:\")\n",
        "    class_metrics = {}\n",
        "    for label in all_labels:\n",
        "        true_mask = y_test == label\n",
        "        pred_mask = y_pred == label\n",
        "\n",
        "        if np.any(true_mask) or np.any(pred_mask):\n",
        "            precision = precision_score(y_test == label, y_pred == label,\n",
        "                                     zero_division=0)\n",
        "            recall = recall_score(y_test == label, y_pred == label,\n",
        "                                zero_division=0)\n",
        "            f1 = f1_score(y_test == label, y_pred == label,\n",
        "                         zero_division=0)\n",
        "\n",
        "            class_metrics[label] = {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'support': np.sum(true_mask)\n",
        "            }\n",
        "\n",
        "            print(f\"\\nClass: {label}\")\n",
        "            print(f\"Precision: {precision:.2f}\")\n",
        "            print(f\"Recall: {recall:.2f}\")\n",
        "            print(f\"F1 Score: {f1:.2f}\")\n",
        "            print(f\"Support: {np.sum(true_mask)}\")\n",
        "\n",
        "    return overall_metrics, class_metrics, history, classifier\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Using the same train-test split as previous tasks\n",
        "overall_metrics, class_metrics, history, model = train_and_evaluate_cnn(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Print overall results\n",
        "print(\"\\nOverall CNN Model Results:\")\n",
        "print(f\"Precision: {overall_metrics['precision']:.2f}\")\n",
        "print(f\"Recall: {overall_metrics['recall']:.2f}\")\n",
        "print(f\"F1 Score: {overall_metrics['f1']:.2f}\")\n",
        "print(\"Confusion Matrix:\\n\", overall_metrics['confusion_matrix'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}